{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bogdan Bintu\n",
    "### Copyright Presidents and Fellows of Harvard College, 2018."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard analysis of the STORM data\n",
    "\n",
    "The subfolders H#R# indicate the hybrdization round and the index of the 30kb region imaged respectively.\n",
    "\n",
    "The region chr21:28Mb-29.2Mb was imaged with 30Kb resolution using STORM across two independent datasets with 100-150 chromosomes each.\n",
    "\n",
    "Each subfolder contains both diffraction-limited and STORM imaged fields of view, each corresponding to a '.dax' file with the fluorescent signal recorded by the camera. \n",
    "This file format can be opened with tools in ChromatinImaging\\CommonTools\\IOTools.py\n",
    "\n",
    "#### Imaging experimental details:\n",
    "\n",
    "For the first round of hybridization in a multiplexed STORM imaging experiment we mixed into the hybridization buffer the following oligonucleotides: 1) the first 30-nt Alexa647-labeled readout probe (at 100 nM) which has homology to the readout sequence on the primary probes targeting the first 30 kb of chromatin (chr21:28000001-28030000) and 2) an Alexa405-labeled activator probe (at 100 nM).\n",
    "\n",
    "For the subsequent rounds we added: 1) an Alexa647-labeled readout probe (100 nM) targeting the readout sequence on the primary probes hybridized to the next 30-kb segment and 2) an unlabeled oligo sequence (1 Î¼M) fully complementary to the previously flowed readout probe. The unlabeled oligo is used to remove the previous readout probe as detailed in the toehold displacement reaction.\n",
    "\n",
    "We used the first 41 rounds of sequential hybridization and imaging of readout probes to target the 41 consecutive 30-kb chromatin segments in the genomic region of interest. After this, we used additional rounds to relabel some of the readout probes for assessing alignment accuracy and sample variation over time. We performed three-dimensional difraction-limited and then STORM imaging after each round of hybridization to each chromatin segment. \n",
    "\n",
    "Imaging of ~18 fields of view for the 41 segments took approximately 5 days in total. \n",
    "\n",
    "The 3-valve fluidics system that we constructed allowed for the loading of only 20 different hybridization solutions. Consequently, after every 20 rounds we short circuited the fluidics chamber, washed each of the 20 channels with 30% formamide in water and loaded new hybridization solutions. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Script used for STORM analysis\n",
    "#### Each notebook sections should be performed sequentially. Note: some parts require GUI input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Imports\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import GeneralToolsSTORMTracing as gtct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1\n",
    "### Get a list of all .dax files corresponding to STORM movies residing in the first hybe folder.\n",
    "### Start a series of GUIs which allow selecting the chromosomal positions based on the Cy3 signal in the first hybe of the STORM movies.\n",
    "\n",
    "### Naming convention:\n",
    "### A master folder contains subfolders labelled H#iR#j.  (hybe round #i, region #j)\n",
    "### The subfolders contain matching STORM and difraction-limited imaging files labelled STORM_#k.dax and Conv_zscan_#k.dax where #k indicates the field of view number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "first_hybe_folder = r\"\" #### update\n",
    "list_daxes = gt.listDax(first_hybe_folder,name='STORM_')\n",
    "dic_dax = [gtct.STORM_STD_pointSelect(dax_fl) for dax_fl in list_daxes[:]]\n",
    "\n",
    "## Decide on a master_save_folder and save the selected positionssave_folder = master_save_folder+os.sep+'selectedSpots'\n",
    "if not os.path.exists(save_folder): os.makedirs(save_folder)\n",
    "big_dic={}\n",
    "for dic in dic_dax:\n",
    "    big_dic.update(dic)\n",
    "for dax_fl in big_dic.keys():\n",
    "    imshow_obj = big_dic[dax_fl]\n",
    "    #save coords\n",
    "    dic_name = gtct.save_name_dic(dax_fl,save_folder=master_save_folder)\n",
    "    pickle.dump(imshow_obj.coords,open(dic_name[\"coords_file\"],'w'))\n",
    "    #save figure\n",
    "    imshow_obj.fig.set_size_inches(12, 12)\n",
    "    imshow_obj.fig.savefig(dic_name[\"png_figure\"], dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2\n",
    "#### Once a round of hybridization and imaging is complete the corresponding subfolder H#iR#j folder is copied automatically from the solid state drive to one of 3 8Tb drives available.\n",
    "#### Get pointers to these files and start aligning them to pixel precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "drive_folders = [r'drive1',\n",
    "                 r'drive2',\n",
    "                 r'drive3'] ### update\n",
    "#List all the dax\n",
    "dax_files=[]\n",
    "for data_folder in drive_folders:\n",
    "    dax_files += gt.listDax(data_folder,name='STORM_')\n",
    "#Next partition according to field of view and sort according to the order of hybridization\n",
    "fovs = [gt.extract_flag(dax_file,'STORM_','.') for dax_file in dax_files]\n",
    "def sort_to_hybe(dax_fls):\n",
    "    r'given a list of dax files containing the hybe folder info \\H<index>R...\\it returns an sorted by index list of dax files'\n",
    "    map_=[int(dax_fl.split(os.sep+'H')[-1].split('R')[0]) for dax_fl in dax_fls]\n",
    "    return list(np.array(dax_fls)[np.argsort(map_)])\n",
    "dax_files_fovs = map(sort_to_hybe,gt.partition_map(dax_files,fovs))\n",
    "\n",
    "#save file names for convenience in a subfolder called roughAlingment\n",
    "save_folder = master_save_folder+os.sep+'roughAlingment'\n",
    "if not os.path.exists(save_folder): os.makedirs(save_folder)\n",
    "pickle.dump(dax_files_fovs,open(save_folder+os.sep+'dax_files_fovs.pkl','w'))\n",
    "\n",
    "## Perform rough registration and save results.\n",
    "\n",
    "\n",
    "ind_ref = 0 #The hybe index to align to. By default everything is aligned to the first hybe.\n",
    "dic_registration={} \n",
    "coords_dic={}\n",
    "refs_dic={}\n",
    "\n",
    "# If failed or decide to restart, load previously saved results.\n",
    "dic_registration_fl = save_folder+os.sep+'dic_registration.pkl'\n",
    "if os.path.exists(dic_registration_fl):\n",
    "    dic_registration=pickle.load(open(dic_registration_fl,'r'))\n",
    "    coords_dic=pickle.load(open(save_folder+os.sep+'coords_dic.pkl','r'))\n",
    "    refs_dic=pickle.load(open(save_folder+os.sep+'refs_dic.pkl','r'))\n",
    "\n",
    "for dax_files_fov in dax_files_fovs:\n",
    "    nm0 = dax_files_fov[ind_ref]\n",
    "    im0 = gtct.load_cy3(nm0,func=np.max)\n",
    "    for nm1 in dax_files_fov[:]:\n",
    "        basenm0 = \"--\".join(nm0.split(os.sep)[-2:])\n",
    "        basenm1 = \"--\".join(nm1.split(os.sep)[-2:])\n",
    "        key_nm = basenm0+'<-'+basenm1\n",
    "        print key_nm\n",
    "        #if not dic_registration.has_key(key_nm):\n",
    "        if not coords_dic.has_key(nm1):\n",
    "            im1 = gtct.load_cy3(nm1,func=np.max)\n",
    "            txy = gtct.fftalign_guess(im0,im1,center=[0,0],max_disp=50) # this is the main function.  Register everyone to the ref field of view using fft correlation.\n",
    "            dic_registration[key_nm] = txy # collect to dictionary\n",
    "\n",
    "            coord_file = gtct.save_name_dic(nm0,master_save_folder)[\"coords_file\"]\n",
    "            if os.path.exists(coord_file):\n",
    "                coords = pickle.load(open(coord_file,'r'))\n",
    "                coords_dic[nm1]=np.array(coords)-np.array([txy[::-1]])\n",
    "                refs_dic[nm1] = np.array(coords)\n",
    "            else:\n",
    "                coords_dic[nm1] = coords_dic[nm0]-np.array([txy[::-1]])\n",
    "                refs_dic[nm1] = refs_dic[nm0]\n",
    "            #save figure\n",
    "            fig = plt.figure(figsize=(12,12))\n",
    "            ax = fig.add_subplot(111)\n",
    "            xy = zip(*coords_dic[nm1])\n",
    "            if len(xy)>0:\n",
    "                ax.plot(np.array(xy[0])-1,np.array(xy[1])-1, 'o',markersize=12,markeredgewidth=1,markeredgecolor='g',markerfacecolor='None')\n",
    "                for i_txt,(x_,y_) in enumerate(coords_dic[nm1]):\n",
    "                    ax.text(x_-1,y_-1,str(i_txt),color='g')\n",
    "            ax.imshow(im1,cmap=cm.hot,interpolation='nearest')\n",
    "\n",
    "            file_ = \"--\".join(nm1.split(os.sep)[-2:][::-1]).replace('.dax','')+'_cy3-roughalignment.png'\n",
    "            fig.savefig(save_folder+os.sep+file_)\n",
    "            plt.close(fig)\n",
    "            #save dics\n",
    "            pickle.dump(coords_dic,open(save_folder+os.sep+'coords_dic.pkl','w'))\n",
    "            pickle.dump(refs_dic,open(save_folder+os.sep+'refs_dic.pkl','w'))\n",
    "            pickle.dump(dic_registration,open(dic_registration_fl,'w'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
